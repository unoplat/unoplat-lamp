{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f524ed-1582-4b0d-abb9-e77650e21845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install unstructured\n",
    "# ! pip install sentence-transformers\n",
    "# ! pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa1fb4e-1b4f-4b3a-b90d-fb83a554cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "import os\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e517092-6732-4170-8792-2806ce981393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load the markdown files for Mermaid Knowledge Base\n",
    "path = \"/Users/nmg/Desktop/Lamp/mermaid-develop/docs/syntax/\"\n",
    "loader = NotionDirectoryLoader(path)\n",
    "docs = loader.load()\n",
    "\n",
    "md_file = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68ac9c1f-82dd-4d1a-8787-afc67d5bd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Main Header\"),\n",
    "    (\"##\", \"Section\"),\n",
    "    (\"###\", \"Sub Section\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "febb8328-98c9-4bef-a1f7-ce97a3924ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_header_splits_list= []\n",
    "meta_data_types = []\n",
    "for doc in docs:\n",
    "    md_file = doc.page_content\n",
    "    md_header_splits = markdown_splitter.split_text(md_file)\n",
    "    filename = os.path.basename(doc.metadata[\"source\"])\n",
    "    diagram_type = filename.split(\".\")[0]\n",
    "    meta_data_types.append(diagram_type)\n",
    "    updated_splits = []\n",
    "    for chunk in md_header_splits:\n",
    "        existing_metadata = chunk.metadata.copy()  # Create a copy of the existing metadata\n",
    "        existing_metadata[\"DiagramType\"] = diagram_type\n",
    "        chunk.metadata = existing_metadata # Update with new metadata\n",
    "        updated_splits.append(chunk)\n",
    "    md_header_splits_list.append(updated_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "600e07ce-c646-40bc-a13e-1191d139f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# md_header_splits_list[16][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb8facbc-ba50-4218-951d-998dc6f28ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "\n",
    "for splits in md_header_splits_list:\n",
    "    for chunk in splits:\n",
    "        length = len(chunk.page_content)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "\n",
    "# print(\"Maximum length of md_header_splits:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5789461a-3d8b-4a01-8112-219462712abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split the chunks into smaller pieces while preserving metadata\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Define the maximum chunk size for the document transformer\n",
    "max_chunk_size = 512\n",
    "\n",
    "# Create an instance of the RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=max_chunk_size, chunk_overlap=0)\n",
    "\n",
    "\n",
    "\n",
    "# Split the chunks into smaller pieces while preserving metadata\n",
    "split_chunks = []\n",
    "for splits in md_header_splits_list:\n",
    "    for chunk in splits:\n",
    "        if len(chunk.page_content) > max_chunk_size:\n",
    "            smaller_chunks = text_splitter.split_text(chunk.page_content)\n",
    "            for smaller_chunk in smaller_chunks:\n",
    "                split_chunk = Document(page_content=smaller_chunk, metadata=chunk.metadata.copy())\n",
    "                split_chunks.append(split_chunk)\n",
    "        else:\n",
    "            split_chunk = Document(page_content=chunk.page_content, metadata=chunk.metadata.copy())\n",
    "            split_chunks.append(split_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83812ac4-3be1-4b68-9512-af07897bca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(split_chunks[15].page_content)\n",
    "# print(split_chunks[15].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1f1405c-8204-4733-aff7-c4536544b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#use this model for text corpus\n",
    "modelPath = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'mps'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "# this experiment based on usage\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0af22728-3275-431f-b640-ae718d9687f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(split_chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ef802f7-86d5-4c3b-a064-c3abb5c57c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af72f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt\n",
    "template = \"\"\"Instruction: Use the following pieces of context to answer the question at the end. \n",
    "Provided a question {question}, Detect the type of diagram to be generated from the list \"\"\" + \",\".join(meta_data_types) + \"\"\"\n",
    "and output the detected type. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Output: Provide a single word reply.\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f49c73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model1 = ChatOllama(model=\"codellama:34b-instruct\", \n",
    "                        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2840bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Flowchart"
     ]
    }
   ],
   "source": [
    "# Chain\n",
    "question = \"How do I draw a top down flow chart. Write an example for the same?\"\n",
    "llm_chain = LLMChain(\n",
    "    llm=chat_model1,\n",
    "    prompt=QA_CHAIN_PROMPT\n",
    ")\n",
    "\n",
    "meta_data_type = llm_chain(question, return_only_outputs=True)['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a02a733b-4a03-4773-bff3-29e45d9b0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Instruction: Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Output: Use three sentences maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT2 = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a727df56-e5c6-4355-94b8-a13ce125b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_model2 = ChatOllama(model=\"codellama:34b-instruct\", \n",
    "                        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f85e4e8-0ade-46bd-9813-e6f9131980f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "metadata_filter = {\"DiagramType\": meta_data_type}\n",
    "\n",
    "docs = retriever.get_relevant_documents(question,metadata_filter=metadata_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b271e780-eed1-4ca2-8bc4-bb38288e3d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To draw a top-down flowchart, you can use the `flowchart TD` diagram type in Mermaid. Here's an example:\n",
      "```mermaid\n",
      "flowchart TD\n",
      "    A[Start] --> B[Do something]\n",
      "    B --> C[Do something else]\n",
      "    C --> D[End]\n",
      "```\n",
      "This will create a top-down flowchart with four nodes (A, B, C, and D) connected by arrows. The `TD` in the diagram type stands for \"top-down,\" which indicates that the chart should be drawn from top to bottom."
     ]
    }
   ],
   "source": [
    "# Chain\n",
    "chain = load_qa_chain(chat_model2, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT2)\n",
    "\n",
    "# Run\n",
    "new_output = chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)['output_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
